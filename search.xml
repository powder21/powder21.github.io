<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>WGAN-GP训练出现NaN</title>
      <link href="/WGAN-GP%E8%AE%AD%E7%BB%83%E5%87%BA%E7%8E%B0NaN/"/>
      <url>/WGAN-GP%E8%AE%AD%E7%BB%83%E5%87%BA%E7%8E%B0NaN/</url>
      <content type="html"><![CDATA[<p>最近在做实验时需要用到WGAN-GP，训练时偶尔会出现NaN，今天仔细检查了一下，找出了原因：gradient penalty中有开根号操作，求导时可能会出现除以0，出现NaN</p><p>先看一下WGAN-GP用tensorflow的实现代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wgan_loss</span><span class="params">(real, fake, name=<span class="string">'wgan_loss'</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    wgan loss function for GANs.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - Wasserstein GAN: https://arxiv.org/abs/1701.07875</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">        d_loss = tf.reduce_mean(fake-real)</span><br><span class="line">        g_loss = -tf.reduce_mean(fake)</span><br><span class="line">    <span class="keyword">return</span> g_loss, d_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_interpolates</span><span class="params">(x, y, alpha=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    x: first dimension as batch_size</span></span><br><span class="line"><span class="string">    y: first dimension as batch_size</span></span><br><span class="line"><span class="string">    alpha: [BATCH_SIZE, 1]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    shape = x.get_shape().as_list()</span><br><span class="line">    x = tf.reshape(x, [shape[<span class="number">0</span>], <span class="number">-1</span>])</span><br><span class="line">    y = tf.reshape(y, [shape[<span class="number">0</span>], <span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">if</span> alpha <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        alpha = tf.random_uniform(shape=[shape[<span class="number">0</span>], <span class="number">1</span>])</span><br><span class="line">    interpolates = x + alpha*(y - x)</span><br><span class="line">    <span class="keyword">return</span> tf.reshape(interpolates, shape)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradients_penalty</span><span class="params">(x, y, mask=None, norm=<span class="number">1.</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Improved Training of Wasserstein GANs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    gradients = tf.gradients(y, x)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        mask = tf.ones_like(gradients)</span><br><span class="line">    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients) * mask, axis=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.square(slopes - norm))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">g_loss, d_loss = wgan_loss(real_d_logits, fake_d_logits)</span><br><span class="line">fake_data = G_net(random_z)</span><br><span class="line">interpolates = random_interpolates(real_data, fake_data)</span><br><span class="line">inter_d_logits = D_net(interpolates)</span><br><span class="line">penalty = gradients_penalty(interpolates, inter_d_logits)</span><br><span class="line"></span><br><span class="line">d_loss += <span class="keyword">lambda</span> * penalty</span><br></pre></td></tr></table></figure></p><p>注意到在gradients_penalty函数中有使用<code>tf.sqrt</code>，求导时可能会有除以0出现，从而发生NaN，改为如下即可：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradients_penalty</span><span class="params">(x, y, mask=None, norm=<span class="number">1.</span>)</span>:</span></span><br><span class="line">    <span class="string">"""Improved Training of Wasserstein GANs</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    - https://arxiv.org/abs/1704.00028</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    gradients = tf.gradients(y, x)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        mask = tf.ones_like(gradients)</span><br><span class="line">    slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients) * mask, axis=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]) + <span class="number">1e-8</span>)</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(tf.square(slopes - norm))</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</title>
      <link href="/Process-finished-with-exit-code-137-interrupted-by-signal-9-SIGKILL/"/>
      <url>/Process-finished-with-exit-code-137-interrupted-by-signal-9-SIGKILL/</url>
      <content type="html"><![CDATA[<p>今天发现在GPU集群的一组程序全都异常停止了，没有报错，而且停的时间也都不一样。到本地运行相同的程序发现报错<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Process finished with exit code 137 (interrupted by signal 9: SIGKILL)</span><br></pre></td></tr></table></figure></p><p>查了一下，猜测可能是两个原因：<br>1.CPU占用<br>2.内存占用<br>用top指令查看程序运行时的两项占用，CPU正常，内存在每个epoch结束之后会飙升至32G，其他时候16G左右。最终定位到是数据打乱时的<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.p == <span class="number">0</span> <span class="keyword">and</span> self.shuffle:</span><br><span class="line">    inds = self.rng.permutation(self.data.shape[<span class="number">0</span>])</span><br><span class="line">    self.data = self.data[inds]</span><br></pre></td></tr></table></figure></p><p>最后一句赋值时会生成一个备份，导致内存溢出。更改为<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> self.p == <span class="number">0</span> <span class="keyword">and</span> self.shuffle:</span><br><span class="line">    np.random.shuffle(self.data)</span><br></pre></td></tr></table></figure></p><p>shuffle函数只会打乱array的第一个维度，并且由于采用算法原因不用整个数据的备份，空间占用小。</p>]]></content>
      
      
    </entry>
    
    <entry>
      <title>tensorflow保存和加载网络参数</title>
      <link href="/tensorflow%E5%8A%A0%E8%BD%BD%E6%8C%87%E5%AE%9A%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0/"/>
      <url>/tensorflow%E5%8A%A0%E8%BD%BD%E6%8C%87%E5%AE%9A%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0/</url>
      <content type="html"><![CDATA[<h3 id="保存参数"><a href="#保存参数" class="headerlink" title="保存参数"></a>保存参数</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">saver = tf.train.Saver()</span><br><span class="line">saver.save(sess, <span class="string">'model.ckpt'</span>)</span><br></pre></td></tr></table></figure><h3 id="加载全部参数（默认）"><a href="#加载全部参数（默认）" class="headerlink" title="加载全部参数（默认）"></a>加载全部参数（默认）</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">saver.restore(sess, <span class="string">'model.ckpt'</span>)</span><br></pre></td></tr></table></figure><h3 id="加载指定参数"><a href="#加载指定参数" class="headerlink" title="加载指定参数"></a>加载指定参数</h3><p><strong>ckpt文件中包含了在训练中所有的参数，但是在test时可能只需要使用部分网络就可以，这时候就需要只加载部分变量。</strong><br>指定的方式就是根据变量名进行区分，所以在训练之前声明各个变量时scope应该区分好，尽可能自己命名变量的name参数。<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">w = tf.get_variable(<span class="string">'w'</span>, shape, initializer)</span><br><span class="line"><span class="keyword">with</span> tf.variable_scope(name):</span><br><span class="line">conv1 = conv2d(input)</span><br><span class="line">conv2 = conv2d(conv1)</span><br><span class="line">...</span><br></pre></td></tr></table></figure></p><p>之后就可以从.ckpt中得到变量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 几种variable</span></span><br><span class="line">tf.contrib.framework.get_variables_to_restore()</span><br><span class="line">tf.all_variables()</span><br><span class="line">tf.trainable_varialbes()</span><br></pre></td></tr></table></figure></p><p>根据变量名加载指定变量<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">variables = trainable_varialbes <span class="comment"># 所有可加载的变量</span></span><br><span class="line">variable_to_restore = [v <span class="keyword">for</span> v <span class="keyword">in</span> variables <span class="keyword">if</span> <span class="string">'xxx'</span> <span class="keyword">in</span> v.name] <span class="comment"># if 后是任意可筛选的条件即可</span></span><br><span class="line">saver = tf.train.Saver(variable_to_restore)</span><br><span class="line">saver.restore(sess, <span class="string">'model.ckpt'</span>)</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
        <tags>
            
            <tag> tensorflow </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>博客搭好了，纪念一下</title>
      <link href="/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%A5%BD%E4%BA%86%EF%BC%8C%E7%BA%AA%E5%BF%B5%E4%B8%80%E4%B8%8B/"/>
      <url>/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%A5%BD%E4%BA%86%EF%BC%8C%E7%BA%AA%E5%BF%B5%E4%B8%80%E4%B8%8B/</url>
      <content type="html"><![CDATA[<p>昨晚想起来博客这件事已经是10点，一开始入手jekyll，但总是有同步慢、网页404等一系列问题。今天转入Hexo，感觉好用很多。花了一下午加一晚上配置好了一些布局，之后可以专注于写一些博客了。</p><h3 id="下面是hex的主要语法"><a href="#下面是hex的主要语法" class="headerlink" title="下面是hex的主要语法"></a>下面是hex的主要语法</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new ***</span><br></pre></td></tr></table></figure><p>会在source/_post下建立***.md文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean</span><br><span class="line">hexo g</span><br><span class="line">hexo d</span><br></pre></td></tr></table></figure></p><p>是将本地更改上传至github、站点生效的命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server</span><br></pre></td></tr></table></figure></p><p>可以在本地预览变更，但个别变更如：description必须push至github才能生效</p><h3 id="下面是一些参考网址"><a href="#下面是一些参考网址" class="headerlink" title="下面是一些参考网址"></a>下面是一些参考网址</h3><ol><li><a href="https://reuixiy.github.io/technology/computer/computer-aided-art/2017/06/09/hexo-next-optimization.html" target="_blank" rel="noopener">配置及说明</a></li><li><a href="https://www.zybuluo.com/mdeditor?url=https://www.zybuluo.com/static/editor/md-help.markdown" target="_blank" rel="noopener">markdown语法</a></li><li><a href="https://www.zybuluo.com/codeep/note/163962#8%E5%A6%82%E4%BD%95%E8%BE%93%E5%85%A5%E7%A7%AF%E5%88%86" target="_blank" rel="noopener">markdown公式</a></li></ol>]]></content>
      
      
        <tags>
            
            <tag> 日常 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
