<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Exponential Moving Average in Tensorflow]]></title>
    <url>%2FExponential-Moving-Average-in-Tensorflow.html%2F</url>
    <content type="text"><![CDATA[什么是Exponential Moving Average(EMA)在测试model时，使用过去的weight去平滑最终的weight，平滑方式为指数平滑:$$w_{t+1}=\lambda w_{t}+(1-\lambda) \theta_{t+1}$$可以看出，t时刻的变量和之前很长时间内的历史值都相关，并且相关系数呈指数下降。很多实验表面使用EMA可以提升测试的performance。 tensorflow中EMA的使用在tensorflow中，有现成的EMA接口tf.train.ExponentialMovingAverage完成相关的操作123var = tf.trainable_variables()ema = tf.train.ExponentialMovingAverage(decay=0.999)ema_op = ema.apply(var) 至此在model中会生成额外的用上面提到的公式计算的EMA变量。 关键是在test时使用EMA变量，官方提出了两种可行方案，一种是restore时将EMA参量load进model中，这种方法没有办法在train的时候进行test(validation);另一种是使用ema.average()接口，下面对后者进行说明。主要手段是使用variable scope的getter参量:1234567891011121314151617181920def get_getter(ema): def ema_getter(getter, name, *args, **kwargs): var = getter(name, *args, **kwargs) ema_var = ema.average(var) return ema_var if ema_var else var return ema_getter def discriminator(self, input, n_channel=64, name="discriminator", reuse=False, getter=None): with tf.variable_scope(name, reuse=reuse, custom_getter=getter): with arg_scope([conv2d], padding='SAME', kernel_size=5, stride=2): conv1 = conv2d(input, n_channel * 1, scope="conv1") conv2 = conv2d(conv1, n_channel * 2, scope="conv2") conv3 = conv2d(conv2, n_channel * 4, scope="conv3") conv4 = conv2d(conv3, n_channel * 4, scope="conv4") flatten = tf.contrib.layers.flatten(conv4) output = linear(flatten, 1, name="linear") return [conv1,conv2,conv3,conv4],outputgetter = get_getter(ema)fake_layers, fake_d_logits = discriminator(fake_imgs, getter=getter) get_getter函数主要是返回一个getter给tf.variable_scope的custom_getter参数。当调用tf.get_variable时，默认getter会根据变量名得到已有变量。自定义getter可以修改已有的变量。 注意，training的代码不用变化，getter参量传入None即可 总结EMA是test期间对model从时间维度进行平滑的优化操作，train过程不变，只是会生成平滑的变量存入model中。test情形有两种：在线和离线。离线操作只需在restore时只load平滑后的网络参数，在线操作涉及到ema.average()接口。 参考http://ruishu.io/2017/11/22/ema/https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAveragehttps://github.com/tensorflow/models/blob/master/tutorials/image/cifar10/cifar10_eval.pyhttps://www.tensorflow.org/api_docs/python/tf/get_variable]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用TFRecord处理大数据集IO]]></title>
    <url>%2F%E4%BD%BF%E7%94%A8TFRecord%E5%A4%84%E7%90%86%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9B%86IO.html%2F</url>
    <content type="text"><![CDATA[之前一直使用BSDS500的augment dataset做训练集，28800张图片，全部读进内存大约占10GB，可以实现。但当该数据集可能会过拟合，所以试图使用更大的自然数据集places2，仅训练集就有180w张图片，jpeg格式就占20GB+容量，读进内存里（np.array）会翻好几倍，所以只能在硬盘中每次读取一部分。如果保持原有的数据格式，每个epoch相当于遍历读取一遍训练集，肯定会占用大量时间。所以考虑寻找其他更高效的方式，因为使用tensorflow框架，TFRecord是官方推荐的数据格式，对tensorflow十分友好。下面对此数据读取方式加以总结。 1.Write由于原数据集必然是图片的形式，所以先要将其写入TFRecord格式的文件12345678910111213141516171819202122232425# -*- coding:utf-8 -*-import tensorflow as tfimport numpy as npdef bytes_feature(values): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[values]))# 这里img和edge是一个pairTFRecordFile = 'train.tfrecord'file_to_write_path1 = 'img_path'file_to_write_path2 = 'edge_path'# 需要保证img和edge是匹配的，这里默认匹配for path1, path2 in zip(file_to_write_path1,file_to_write_path2): # 使用tenosrflow接口读取文件，节省内存 img = tf.gfile.FastGFile(path1, 'rb').read() edg = tf.gfile.FastGFile(path2, 'rb').read() with tf.python_io.TFRecordWriter(TFRecordFile) as writer: # 打开TFRecord格式的文件 example = tf.train.Example(features=tf.train.Features(feature=&#123;'image': bytes_feature(img), 'edge': bytes_feature(edg)&#125;)) # 序列化 serialized = example.SerializeToString() # 写入文件 writer.write(serialized) 2.ReadTFRecord文件生成之后就可以作为源提供训练数据123456789101112131415161718192021222324252627282930313233343536# -*- coding:utf-8 -*-import tensorflow as tfimport numpy as np# 将文件分解成队列filename_queue = tf.train.string_input_producer([TFRecordFile], num_epochs=None, shuffle=True)# 读取出序列化的examplereader = tf.TFRecordReader()_, serialized_example = reader.read(filename_queue)# 将example分解成featuresfeatures = tf.parse_single_example(serialized_example, features=&#123;'image': tf.FixedLenFeature([], tf.string), 'edge': tf.FixedLenFeature([], tf.string)&#125;)# 从features中得到tf.tensorimg = features['image']img = tf.image.decode_png(img, channels=3)img = tf.reshape(img, [256, 256, 3])edg = features['edge']edg = tf.image.decode_png(edg, channels=1)edg = tf.reshape(edg, [256, 256, 1])# 以img和edg为例从queue中生成batch。tf.train.batch顺序读取batch，但注意将tf.train.string_input_producer的shuffle=False# 也可以使用tf.train.shuffle_batch生成乱序序列X_batch, Y_batch = tf.train.batch([img, edg], batch_size=100, capacity=200, num_threads=3)sess = tf.Session()init = tf.global_variables_initializer()sess.run(init)# coord 是个线程协调器，把启动队列的时候加上线程协调器coord = tf.train.Coordinator()threads = tf.train.start_queue_runners(sess=sess, coord=coord)# 直接run，注意到这里不需要feed_dict_X_batch, _Y_batch = sess.run([X_batch, Y_batch])coord.request_stop()coord.join(threads)]]></content>
  </entry>
  <entry>
    <title><![CDATA[python logging]]></title>
    <url>%2Fpython-logging.html%2F</url>
    <content type="text"><![CDATA[本文简要介绍python中logging的使用 为什么用logging在写代码时debug，最初用的都是1print(xxxx) 比如下面一段代码：12345678910111213141516171819202122232425262728# -*- coding: utf-8 -*-import argparseparser = argparse.ArgumentParser(description='I print sequence')parser.add_argument('-s', '--start', type=int, dest='start', help='Start of the sequence', required=True)parser.add_argument('-e', '--end', type=int, dest='end', help='End of the sequence', required=True)def array(start, end): i = start yield i while start &lt;= i &lt; end: print('Before interation: i = %s' % (i)) i += 1 print('After interation: i = %s' % (i)) yield idef main(): args = parser.parse_args() for n in array(args.start, args.end): print(n)if __name__ == '__main__': main() 运行结果为12345678910111213141516$ python test_logging.py -s 0 -e 100Current value: 0Before interation: i = 0After interation: i = 1Current value: 1Before interation: i = 1After interation: i = 2Current value: 2Before interation: i = 2After interation: i = 3Current value: 3Before interation: i = 3After interation: i = 4Current value: 4... ...... ... 但是在程序正式release或者投入使用时，上述语句中的print都需要被注释掉。下次我想debug时又得取消注释，这样的过程难免让人觉得反人类。logging模块就可以派上用场，以filter的方式控制当前‘输出级别’，debug时就输出debug需要的琐碎信息，投入使用时就输出正式信息。 怎样用logging最简单的用法是直接使用basicConfig方法来对logging进行配置：12345678910111213141516# -*- coding: utf-8 -*-import logging# 设置默认的level为DEBUG# 设置log的格式logging.basicConfig( level=logging.DEBUG, format="[%(asctime)s] %(name)s:%(levelname)s: %(message)s")# 记录loglogging.debug(...)logging.info(...)logging.warn(...)logging.error(...)logging.critical(...) 上面说的basicConfig方法可以满足你在绝大多数场景下的使用需求，但是basicConfig有一个很大的缺点。调用basicConfig其实是给root logger添加了一个handler，这样当你的程序和别的使用了 logging的第三方模块一起工作时，会影响第三方模块的logger行为。这是由logger的继承特性决定的。所以我们需要使用真正的logger：12345678910111213141516# -*- coding: utf-8 -*-import logging# 使用一个名字为fib的loggerlogger = logging.getLogger(&apos;fib&apos;)# 设置logger的level为DEBUGlogger.setLevel(logging.DEBUG)# 创建一个输出日志到控制台的StreamHandlerhdr = logging.StreamHandler()formatter = logging.Formatter(&apos;[%(asctime)s] %(name)s:%(levelname)s: %(message)s&apos;)hdr.setFormatter(formatter)# 给logger添加上handlerlogger.addHandler(hdr) 最后附上使用logging模块的数列程序完整代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344# -*- coding: utf-8 -*-import argparseparser = argparse.ArgumentParser(description='I logger.debug sequence')parser.add_argument('-s', '--start', type=int, dest='start', help='Start of the sequence', required=True)parser.add_argument('-e', '--end', type=int, dest='end', help='End of the sequence', required=True)parser.add_argument('-v', '--verbose', action='store_true', dest='verbose', help='Enable debug info')import logginglogger = logging.getLogger('array')logger.setLevel(logging.DEBUG)hdr = logging.StreamHandler()formatter = logging.Formatter('[%(asctime)s] %(name)s:%(levelname)s: %(message)s')hdr.setFormatter(formatter)logger.addHandler(hdr)def array(start, end): i = start yield i while start &lt;= i &lt; end: logger.debug('Before interation: i = %s' % (i)) i += 1 logger.debug('After interation: i = %s' % (i)) yield idef main(): args = parser.parse_args() if args.verbose: logger.setLevel(logging.DEBUG) else: logger.setLevel(logging.ERROR) for n in array(args.start, args.end): print('Current value: %s' % n)if __name__ == '__main__': main() 输出如下：123456789101112Current value: 0[2018-09-12 19:43:03,953] array:DEBUG: Before interation: i = 0[2018-09-12 19:43:03,955] array:DEBUG: After interation: i = 1Current value: 1[2018-09-12 19:43:04,339] array:DEBUG: Before interation: i = 1[2018-09-12 19:43:04,340] array:DEBUG: After interation: i = 2Current value: 2[2018-09-12 19:43:04,796] array:DEBUG: Before interation: i = 2[2018-09-12 19:43:04,797] array:DEBUG: After interation: i = 3Current value: 3[2018-09-12 19:43:05,200] array:DEBUG: Before interation: i = 3[2018-09-12 19:43:05,201] array:DEBUG: After interation: i = 4]]></content>
  </entry>
  <entry>
    <title><![CDATA[WGAN-GP训练出现NaN]]></title>
    <url>%2FWGAN-GP%E8%AE%AD%E7%BB%83%E5%87%BA%E7%8E%B0NaN.html%2F</url>
    <content type="text"><![CDATA[最近在做实验时需要用到WGAN-GP，训练时偶尔会出现NaN，今天仔细检查了一下，找出了原因：gradient penalty中有开根号操作，求导时可能会出现除以0，出现NaN 先看一下WGAN-GP用tensorflow的实现代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546def wgan_loss(real, fake, name='wgan_loss'): """ wgan loss function for GANs. - Wasserstein GAN: https://arxiv.org/abs/1701.07875 """ with tf.variable_scope(name): d_loss = tf.reduce_mean(fake-real) g_loss = -tf.reduce_mean(fake) return g_loss, d_lossdef random_interpolates(x, y, alpha=None): """ x: first dimension as batch_size y: first dimension as batch_size alpha: [BATCH_SIZE, 1] """ shape = x.get_shape().as_list() x = tf.reshape(x, [shape[0], -1]) y = tf.reshape(y, [shape[0], -1]) if alpha is None: alpha = tf.random_uniform(shape=[shape[0], 1]) interpolates = x + alpha*(y - x) return tf.reshape(interpolates, shape)def gradients_penalty(x, y, mask=None, norm=1.): """Improved Training of Wasserstein GANs - https://arxiv.org/abs/1704.00028 """ gradients = tf.gradients(y, x)[0] if mask is None: mask = tf.ones_like(gradients) slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients) * mask, axis=[1, 2, 3])) return tf.reduce_mean(tf.square(slopes - norm)) g_loss, d_loss = wgan_loss(real_d_logits, fake_d_logits)fake_data = G_net(random_z)interpolates = random_interpolates(real_data, fake_data)inter_d_logits = D_net(interpolates)penalty = gradients_penalty(interpolates, inter_d_logits)d_loss += lambda * penalty 注意到在gradients_penalty函数中有使用tf.sqrt，求导时可能会有除以0出现，从而发生NaN，改为如下即可：12345678910def gradients_penalty(x, y, mask=None, norm=1.): """Improved Training of Wasserstein GANs - https://arxiv.org/abs/1704.00028 """ gradients = tf.gradients(y, x)[0] if mask is None: mask = tf.ones_like(gradients) slopes = tf.sqrt(tf.reduce_sum(tf.square(gradients) * mask, axis=[1, 2, 3]) + 1e-8) return tf.reduce_mean(tf.square(slopes - norm))]]></content>
  </entry>
  <entry>
    <title><![CDATA[Process finished with exit code 137 (interrupted by signal 9: SIGKILL)]]></title>
    <url>%2FProcess-finished-with-exit-code-137-interrupted-by-signal-9-SIGKILL.html%2F</url>
    <content type="text"><![CDATA[今天发现在GPU集群的一组程序全都异常停止了，没有报错，而且停的时间也都不一样。到本地运行相同的程序发现报错1Process finished with exit code 137 (interrupted by signal 9: SIGKILL) 查了一下，猜测可能是两个原因：1.CPU占用2.内存占用用top指令查看程序运行时的两项占用，CPU正常，内存在每个epoch结束之后会飙升至32G，其他时候16G左右。最终定位到是数据打乱时的123if self.p == 0 and self.shuffle: inds = self.rng.permutation(self.data.shape[0]) self.data = self.data[inds] 最后一句赋值时会生成一个备份，导致内存溢出。更改为12if self.p == 0 and self.shuffle: np.random.shuffle(self.data) shuffle函数只会打乱array的第一个维度，并且由于采用算法原因不用整个数据的备份，空间占用小。]]></content>
  </entry>
  <entry>
    <title><![CDATA[tensorflow保存和加载网络参数]]></title>
    <url>%2Ftensorflow%E5%8A%A0%E8%BD%BD%E6%8C%87%E5%AE%9A%E7%BD%91%E7%BB%9C%E5%8F%82%E6%95%B0.html%2F</url>
    <content type="text"><![CDATA[保存参数12saver = tf.train.Saver()saver.save(sess, 'model.ckpt') 加载全部参数（默认）1saver.restore(sess, 'model.ckpt') 加载指定参数ckpt文件中包含了在训练中所有的参数，但是在test时可能只需要使用部分网络就可以，这时候就需要只加载部分变量。指定的方式就是根据变量名进行区分，所以在训练之前声明各个变量时scope应该区分好，尽可能自己命名变量的name参数。123456with tf.variable_scope(name): w = tf.get_variable('w', shape, initializer)with tf.variable_scope(name): conv1 = conv2d(input) conv2 = conv2d(conv1) ... 之后就可以从.ckpt中得到变量1234# 几种variabletf.contrib.framework.get_variables_to_restore()tf.all_variables()tf.trainable_varialbes() 根据变量名加载指定变量1234variables = trainable_varialbes # 所有可加载的变量variable_to_restore = [v for v in variables if 'xxx' in v.name] # if 后是任意可筛选的条件即可saver = tf.train.Saver(variable_to_restore)saver.restore(sess, 'model.ckpt')]]></content>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搭好了，纪念一下]]></title>
    <url>%2F%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%A5%BD%E4%BA%86%EF%BC%8C%E7%BA%AA%E5%BF%B5%E4%B8%80%E4%B8%8B.html%2F</url>
    <content type="text"><![CDATA[昨晚想起来博客这件事已经是10点，一开始入手jekyll，但总是有同步慢、网页404等一系列问题。今天转入Hexo，感觉好用很多。花了一下午加一晚上配置好了一些布局，之后可以专注于写一些博客了。 下面是hex的主要语法1hexo new *** 会在source/_post下建立***.md文件123hexo cleanhexo ghexo d 是将本地更改上传至github、站点生效的命令1hexo server 可以在本地预览变更，但个别变更如：description必须push至github才能生效 下面是一些参考网址 配置及说明 markdown语法 markdown公式]]></content>
      <tags>
        <tag>日常</tag>
      </tags>
  </entry>
</search>
